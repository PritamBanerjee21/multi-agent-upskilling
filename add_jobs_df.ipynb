{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Annotated, Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")\n",
    "gemini_api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import create_web_crawler_and_study_materials_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=create_web_crawler_and_study_materials_agent(model=\"gemini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jobs(BaseModel):\n",
    "    company: str = Field(description=\"Extract the name of the company that posted the job.\",default=\"Not Specified\")\n",
    "    location: str = Field(description=\"Extract the location.\",default=\"remote\")\n",
    "    job_role: str = Field(description=\"Extract the job role.\",default=\"Not Specified\")\n",
    "    skills_required: list[str] = Field(description=\"Extract all the skills (technical and non-technical)required for the particular job.\",default=\"Not specified\")\n",
    "    salary: str = Field(description=\"Extract the salary the company is offering for that particukar job.\",default=\"Not Specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "parser=PydanticOutputParser(pydantic_object=Jobs)\n",
    "str_parser=StrOutputParser()\n",
    "\n",
    "text=\"\"\"\n",
    "    SOHAM MUKHERJEE\n",
    "Kolkata, India | +91 8240771772 | soumyapu31@gmail.com | LinkedIn | Website | GitHub\n",
    "SUMMARY\n",
    "Aspiring Machine Learning & AI engineer with expertise in Deep Learning and Generative AI. Strong foundation in Neural Networks, Computer Vision, and NLP, with a focus on building intelligent and scalable AI solutions. Passionate about pushing the boundaries of AI-driven innovation. Also, a quick learner and a problem solver.\n",
    "SKILLS\n",
    " Python\n",
    " Computer Vision\n",
    " Transformers\n",
    " RAG\n",
    " MySQL\n",
    " Object Detection\n",
    " LLMs\n",
    " AI Agents\n",
    " Machine Learning\n",
    " NLP\n",
    " Fine-tuning\n",
    " LangChain\n",
    " Neural Networks\n",
    " Embeddings\n",
    " Vector Databases\n",
    " HuggingFace\n",
    "PROJECTS\n",
    "Real-time Google Search AI Agent with Memory | GitHub Repository\n",
    "• Developed a real-time AI-powered Google search agent using LangChain and Groq Inferencing Engine (Llama-3.1-8B model) to retrieve and summarize web data dynamically.\n",
    "• Implemented conversational memory using LangChain's ConversationBufferMemory to retain context for follow-up queries, enhancing response relevance.\n",
    "• Integrated Google Programmable Search API for structured web search results, formatted into markdown for improved readability in a Streamlit-based UI.\n",
    "PDF Question Answering RAG | GitHub Repository\n",
    "• Built a PDF-based Q&A system using Google's Gemini API to extract and answer questions from documents.\n",
    "• Integrated LangChain for efficient document processing and retrieval-augmented generation (RAG).\n",
    "• Developed a user-friendly Streamlit UI for seamless interaction and question-answering.\n",
    "Facial Recognition Attendance System | GitHub Repository\n",
    "• Used OpenCV’s Haar cascades to detect faces and face_recognition library to generate 128-dimensional embeddings for each face.\n",
    "• Compared embeddings with a stored database using Euclidean distance to accurately identify students.\n",
    "• Stores student data in Firebase Realtime Database and images in Firebase Storage for real-time attendance tracking.\n",
    "Multi-purpose AI Assistant | GitHub Repository\n",
    "• Multimodal AI Q&A System using Google’s Gemini API to handle text, image, and invoice-based queries.\n",
    "• Streamlit Web Interface with dedicated pages for chatbot, text Q&A, vision-based queries, and invoice extraction..\n",
    "• Gemini-Powered Features including a chatbot, invoice data extraction, vision-based answering, and text analysis in Markdown..\n",
    "Automatic License Plate Detector | GitHub Repository\n",
    "• License plate detection after fine-tuning a pre-trained YOLOv8 model on our custom dataset.\n",
    "• Text extraction using Tesseract OCR to recognize the license plate numbers.\n",
    "• Visualizing detected plates and extracted text in the web interface built using Streamlit.\n",
    "Food Detector | GitHub Repository\n",
    "• Initial classification with a CNN baseline model.\n",
    "• Transfer learning with EfficientNet and ResNet, and enhanced via data augmentation.\n",
    "• Fine-tuned pre-trained models to get better results.\n",
    "CERTIFICATIONS\n",
    "• Python for Data Science & Machine Learning [Udemy (URL)]\n",
    "• TensorFlow Developer Certification [Zero to Mastery (URL)]\n",
    "EDUCATION\n",
    "Master of Science (M.Sc.) in Data Science & Analytics (2023-present)\n",
    "NSHM College of Management & Technology\n",
    "Bachelor of Science (B.Sc.) in Data Science (2020-2023)\n",
    "iLEAD\n",
    "\"\"\"\n",
    "\n",
    "model=ChatGroq(\n",
    "    model=\"llama-3.3-70b-specdec\",\n",
    "    api_key=groq_api_key\n",
    ")\n",
    "\n",
    "prompt1=PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        Summarize the entire profile of the candidate in question and extract their projects and skilsl from the resume I give below.\\n\n",
    "        {resume}.\n",
    "    \"\"\",\n",
    "    input_variables=[\"resume\"]\n",
    ")\n",
    "\n",
    "summary_chain=prompt1|model|str_parser\n",
    "summary=summary_chain.invoke({\n",
    "    \"resume\":text\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary of the Candidate:**\n",
      "Soham Mukherjee is an aspiring Machine Learning and AI engineer with expertise in Deep Learning and Generative AI. He has a strong foundation in Neural Networks, Computer Vision, and NLP, with a focus on building intelligent and scalable AI solutions. He is passionate about pushing the boundaries of AI-driven innovation and is a quick learner and problem solver. He is currently pursuing his Master's degree in Data Science & Analytics and has a Bachelor's degree in Data Science.\n",
      "\n",
      "**Skills:**\n",
      "\n",
      "1. Programming languages: Python\n",
      "2. AI and ML: Machine Learning, Deep Learning, Neural Networks, NLP, Computer Vision\n",
      "3. Tools and frameworks: Transformers, RAG, LangChain, HuggingFace, MySQL, Object Detection, LLMs, AI Agents\n",
      "4. Technologies: OpenCV, face_recognition, Firebase Realtime Database, Firebase Storage, Streamlit, Tesseract OCR, YOLOv8, EfficientNet, ResNet\n",
      "5. Databases: Vector Databases, MySQL\n",
      "\n",
      "**Projects:**\n",
      "\n",
      "1. **Real-time Google Search AI Agent with Memory**: Developed a real-time AI-powered Google search agent using LangChain and Groq Inferencing Engine to retrieve and summarize web data dynamically.\n",
      "2. **PDF Question Answering RAG**: Built a PDF-based Q&A system using Google's Gemini API to extract and answer questions from documents.\n",
      "3. **Facial Recognition Attendance System**: Used OpenCV and face_recognition library to detect faces and generate 128-dimensional embeddings for each face, with a Firebase Realtime Database and Storage for real-time attendance tracking.\n",
      "4. **Multi-purpose AI Assistant**: Developed a multimodal AI Q&A System using Google's Gemini API to handle text, image, and invoice-based queries, with a Streamlit Web Interface.\n",
      "5. **Automatic License Plate Detector**: Fine-tuned a pre-trained YOLOv8 model to detect license plates and extracted text using Tesseract OCR.\n",
      "6. **Food Detector**: Developed a food detection system using a CNN baseline model, transfer learning with EfficientNet and ResNet, and fine-tuned pre-trained models.\n",
      "\n",
      "**Certifications:**\n",
      "\n",
      "1. Python for Data Science & Machine Learning (Udemy)\n",
      "2. TensorFlow Developer Certification (Zero to Mastery)\n",
      "\n",
      "**Education:**\n",
      "\n",
      "1. Master of Science (M.Sc.) in Data Science & Analytics (2023-present) - NSHM College of Management & Technology\n",
      "2. Bachelor of Science (B.Sc.) in Data Science (2020-2023) - iLEAD\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt_template=\"\"\"\n",
    "    You are a highly skilled job search assistant named Friday. Your task is to analyze a candidate's resume and search for 5 currently open job postings that best match their profile.\n",
    "\n",
    "    Use up-to-date sources (like online job listings or search engines) to find **real** and **ongoing** job opportunities that fit the candidate’s experience, skills, and qualifications.\n",
    "\n",
    "    For each job posting, provide the following details in a structured format:\n",
    "\n",
    "    1. Company Name  \n",
    "    2. Job Role  \n",
    "    3. Skills Required  \n",
    "    4. Location  \n",
    "    5. Salary (if mentioned; otherwise say \"Not specified\")\n",
    "    6. Application Link (direct link to apply for the job)\n",
    "\n",
    "    Be accurate and do not invent information. Only return actual job postings that are live on the internet right now. Focus on relevance and quality over quantity. Use the tools at your disposal.\n",
    "\n",
    "    Here is the resume:\n",
    "    {resume}\n",
    "\"\"\"\n",
    "\n",
    "# industry_trends=agent.invoke({\"messages\":agent_prompt_template.format(resume=summary)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I was unable to find any relevant job postings using the tools available to me.\n"
     ]
    }
   ],
   "source": [
    "print(industry_trends[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Jobs from completion [{\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"Machine Learning Engineer\", \"skills_required\": [\"Python\", \"Machine Learning\", \"Deep Learning\", \"Neural Networks\", \"NLP\", \"TensorFlow\", \"PyTorch\"], \"salary\": \"Not Specified\"}, {\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"AI Engineer\", \"skills_required\": [\"Python\", \"Machine Learning\", \"Deep Learning\", \"NLP\", \"Computer Vision\", \"Generative AI models (GANs, VAEs, Transformers)\"], \"salary\": \"Not Specified\"}, {\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"Computer Vision Engineer\", \"skills_required\": [\"Python\", \"OpenCV\", \"YOLOv8\", \"EfficientNet\", \"ResNet\", \"Machine Learning\", \"Deep Learning\"], \"salary\": \"Not Specified\"}, {\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"NLP Engineer\", \"skills_required\": [\"Python\", \"Transformers\", \"RAG\", \"LangChain\", \"HuggingFace\", \"NLP techniques\"], \"salary\": \"Not Specified\"}, {\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"Data Scientist\", \"skills_required\": [\"Python\", \"Machine Learning\", \"Deep Learning\", \"Data Analysis\", \"SQL\"], \"salary\": \"Not Specified\"}]. Got: 1 validation error for Jobs\n  Input should be a valid dictionary or instance of Jobs [type=model_type, input_value=[{'company': 'Not Specifi...lary': 'Not Specified'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:28\u001b[39m, in \u001b[36mPydanticOutputParser._parse_obj\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m.pydantic_object, pydantic.BaseModel):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpydantic_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m.pydantic_object, pydantic.v1.BaseModel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\pydantic\\main.py:627\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context)\u001b[39m\n\u001b[32m    626\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Jobs\n  Input should be a valid dictionary or instance of Jobs [type=model_type, input_value=[{'company': 'Not Specifi...lary': 'Not Specified'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      3\u001b[39m prompt2=PromptTemplate(\n\u001b[32m      4\u001b[39m     template=\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m        Structure the following -> \u001b[39m\u001b[38;5;132;01m{skills}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{format_instructions}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     }\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m final_chain=prompt2|model|parser\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m final_result=\u001b[43mfinal_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mskills\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3029\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3027\u001b[39m             \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3028\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3029\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3031\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:193\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    188\u001b[39m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage],\n\u001b[32m    189\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    190\u001b[39m     **kwargs: Any,\n\u001b[32m    191\u001b[39m ) -> T:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    202\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    203\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    204\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    205\u001b[39m             config,\n\u001b[32m    206\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    207\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1927\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1923\u001b[39m     context = copy_context()\n\u001b[32m   1924\u001b[39m     context.run(_set_config_context, child_config)\n\u001b[32m   1925\u001b[39m     output = cast(\n\u001b[32m   1926\u001b[39m         Output,\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m         \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1930\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1933\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1935\u001b[39m     )\n\u001b[32m   1936\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1937\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    395\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:194\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    188\u001b[39m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage],\n\u001b[32m    189\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    190\u001b[39m     **kwargs: Any,\n\u001b[32m    191\u001b[39m ) -> T:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    193\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    197\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    198\u001b[39m             config,\n\u001b[32m    199\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    200\u001b[39m         )\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    202\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    203\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    204\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    205\u001b[39m             config,\n\u001b[32m    206\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    207\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:68\u001b[39m, in \u001b[36mPydanticOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     67\u001b[39m     json_object = \u001b[38;5;28msuper\u001b[39m().parse_result(result)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m partial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:36\u001b[39m, in \u001b[36mPydanticOutputParser._parse_obj\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m     34\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (pydantic.ValidationError, pydantic.v1.ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parser_exception(e, obj) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pydantic v1\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mOutputParserException\u001b[39m: Failed to parse Jobs from completion [{\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"Machine Learning Engineer\", \"skills_required\": [\"Python\", \"Machine Learning\", \"Deep Learning\", \"Neural Networks\", \"NLP\", \"TensorFlow\", \"PyTorch\"], \"salary\": \"Not Specified\"}, {\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"AI Engineer\", \"skills_required\": [\"Python\", \"Machine Learning\", \"Deep Learning\", \"NLP\", \"Computer Vision\", \"Generative AI models (GANs, VAEs, Transformers)\"], \"salary\": \"Not Specified\"}, {\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"Computer Vision Engineer\", \"skills_required\": [\"Python\", \"OpenCV\", \"YOLOv8\", \"EfficientNet\", \"ResNet\", \"Machine Learning\", \"Deep Learning\"], \"salary\": \"Not Specified\"}, {\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"NLP Engineer\", \"skills_required\": [\"Python\", \"Transformers\", \"RAG\", \"LangChain\", \"HuggingFace\", \"NLP techniques\"], \"salary\": \"Not Specified\"}, {\"company\": \"Not Specified\", \"location\": \"remote\", \"job_role\": \"Data Scientist\", \"skills_required\": [\"Python\", \"Machine Learning\", \"Deep Learning\", \"Data Analysis\", \"SQL\"], \"salary\": \"Not Specified\"}]. Got: 1 validation error for Jobs\n  Input should be a valid dictionary or instance of Jobs [type=model_type, input_value=[{'company': 'Not Specifi...lary': 'Not Specified'}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "result=industry_trends[\"messages\"][-1].content\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        Structure the following -> {skills}. \\n{format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"skills\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\":parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "final_chain=prompt2|model|parser\n",
    "final_result=final_chain.invoke({\"skills\":result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfinal_result\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'final_result' is not defined"
     ]
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_agent import create_langgraph_agent\n",
    "\n",
    "agent=create_langgraph_agent(model=\"gemini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"1\"}}\n",
    "events = agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"I'm learning LangGraph. \"\n",
    "                    \"Could you do some research on it for me?\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "outputs=[]\n",
    "# for event in events:\n",
    "#     if \"messages\" in event:\n",
    "#         event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        outputs.extend(event[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Ya that's helpful. Maybe I'll \"\n",
    "                    \"build an autonomous agent with it!\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        outputs.extend(event[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Can you help me do it?\"\n",
    "                    \"Can you give me a step by step walkthrough\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        outputs.extend(event[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can provide a general step-by-step walkthrough to help you get started, but keep in mind that building an autonomous agent is a complex task, and the specific steps will depend on the agent's purpose and design.\n",
      "\n",
      "**I. Project Setup and Initialization**\n",
      "\n",
      "1.  **Install LangGraph and LangChain:**\n",
      "\n",
      "    \n",
      "```bash\n",
      "    pip install langgraph langchain\n",
      "    ```\n",
      "\n",
      "\n",
      "2.  **Set up your environment:** Make sure you have Python installed (preferably 3.8 or higher). You'll also need an API key for a Large Language Model provider like OpenAI. Set the API key as an environment variable:\n",
      "\n",
      "    \n",
      "```bash\n",
      "    export OPENAI_API_KEY=\"YOUR_API_KEY\"\n",
      "    ```\n",
      "\n",
      "\n",
      "**II. Define the Agent's State**\n",
      "\n",
      "1.  **Define the State Class:** The state represents the agent's memory and current understanding. It should include all the information the agent needs to make decisions. For example:\n",
      "\n",
      "    \n",
      "```python\n",
      "    from typing import TypedDict, List, Dict\n",
      "    from langchain.schema import BaseMessage\n",
      "\n",
      "    class AgentState(TypedDict):\n",
      "        messages: List[BaseMessage]\n",
      "        user_goal: str\n",
      "        # Add other relevant state variables like 'tools_available', 'current_task', etc.\n",
      "    ```\n",
      "\n",
      "\n",
      "**III. Define the Agent's Actions (Nodes)**\n",
      "\n",
      "1.  **Create Tool Functions:** Define functions that the agent can use to interact with the world. These could include:\n",
      "\n",
      "    *   A search tool (using Tavily or another search engine).\n",
      "    *   A writing tool (to generate text).\n",
      "    *   A tool to access a database or API.\n",
      "    *   A tool to reflect on its actions.\n",
      "\n",
      "    Example using the available `default_api`:\n",
      "\n",
      "    \n",
      "```python\n",
      "    from langchain.tools import tool\n",
      "\n",
      "    @tool\n",
      "    def search_internet(query: str) -> str:\n",
      "        \"\"\"Searches the internet for relevant information.\"\"\"\n",
      "        return default_api.tavily_search_results_json(query=query)\n",
      "    ```\n",
      "\n",
      "\n",
      "2.  **Create Agent Logic (Nodes):** These are the functions that determine the agent's behavior. They take the current state as input and return actions to take (e.g., call a tool, update the state, decide on the next step).\n",
      "\n",
      "    \n",
      "```python\n",
      "    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
      "    from langchain.chat_models import ChatOpenAI\n",
      "    from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
      "\n",
      "    # Initialize LLM\n",
      "    model = ChatOpenAI(temperature=0.7)\n",
      "\n",
      "    # Define a prompt\n",
      "    prompt = ChatPromptTemplate.from_messages([\n",
      "        SystemMessage(\"You are a helpful AI assistant. You have access to tools to help you answer questions. Reason step by step. \"),\n",
      "        MessagesPlaceholder(variable_name=\"messages\"),\n",
      "        HumanMessage(\"{user_goal}\"),\n",
      "    ])\n",
      "\n",
      "    def agent_node(state: AgentState):\n",
      "        \"\"\"\n",
      "        This node decides what to do based on the current state.\n",
      "        \"\"\"\n",
      "        messages = state['messages']\n",
      "        user_goal = state['user_goal']\n",
      "\n",
      "        # Format the prompt\n",
      "        formatted_prompt = prompt.format_messages(messages=messages, user_goal=user_goal)\n",
      "\n",
      "        # Get the model's response\n",
      "        response = model(formatted_prompt)\n",
      "\n",
      "        # Return the next action\n",
      "        return {\"next\": response}\n",
      "    ```\n",
      "\n",
      "\n",
      "**IV. Define the Graph (Edges and Conditional Transitions)**\n",
      "\n",
      "1.  **Create the Graph:** Use `langgraph.Graph` to define the workflow.\n",
      "\n",
      "    \n",
      "```python\n",
      "    import langgraph as lg\n",
      "    from langchain.schema import AgentAction, AgentFinish\n",
      "    from typing import Union\n",
      "\n",
      "    # Define a Union type for the output of the agent node\n",
      "    AgentOutput = Union[AgentAction, AgentFinish, AIMessage]\n",
      "\n",
      "    graph = lg.Graph[AgentState]()\n",
      "\n",
      "    # Add the agent node\n",
      "    graph.add_node(\"agent\", agent_node)\n",
      "    ```\n",
      "\n",
      "\n",
      "2.  **Add Tool Nodes:** For each tool, create a node that executes the tool and updates the state.\n",
      "\n",
      "    \n",
      "```python\n",
      "    def execute_tool(state):\n",
      "        \"\"\"Executes the tool chosen by the agent.\"\"\"\n",
      "        messages = state[\"messages\"]\n",
      "        # Get the last message, which should be an AgentAction\n",
      "        last_message = messages[-1]\n",
      "        tool_name = last_message.tool\n",
      "        tool_input = last_message.tool_input\n",
      "\n",
      "        # Execute the tool (replace with your actual tool execution logic)\n",
      "        if tool_name == \"search_internet\":\n",
      "            tool_output = search_internet.run(tool_input) # Assuming search_internet is a LangChain Tool\n",
      "        else:\n",
      "            tool_output = \"Tool not found.\"\n",
      "\n",
      "        # Return the tool output as a message\n",
      "        return {\"next\": AIMessage(content=str(tool_output))}\n",
      "\n",
      "    graph.add_node(\"tool\", execute_tool)\n",
      "    ```\n",
      "\n",
      "\n",
      "3.  **Define Edges and Conditional Transitions:**\n",
      "\n",
      "    *   **Conditional Edges:** Determine the flow of the graph based on the agent's output.  For example, if the agent chooses a tool, go to the tool execution node.  If the agent is finished, end the graph.\n",
      "\n",
      "    \n",
      "```python\n",
      "    def should_use_tool(message: AgentOutput):\n",
      "        \"\"\"Determines whether the agent should use a tool or respond directly.\"\"\"\n",
      "        if isinstance(message, AgentAction):\n",
      "            return \"tool\"\n",
      "        elif isinstance(message, AgentFinish):\n",
      "            return \"end\"\n",
      "        else:\n",
      "            return \"agent\" # Re-invoke agent if it didn't return an action\n",
      "\n",
      "    graph.add_conditional_edges(\"agent\", should_use_tool, {\n",
      "        \"tool\": \"tool\",\n",
      "        \"end\": lg.END,\n",
      "        \"agent\": \"agent\" #loop\n",
      "    })\n",
      "\n",
      "    graph.add_edge(\"tool\", \"agent\") # after tool, go back to agent\n",
      "    ```\n",
      "\n",
      "\n",
      "4.  **Entrypoint:** Designate the starting node.\n",
      "\n",
      "    \n",
      "```python\n",
      "    graph.set_entry_point(\"agent\")\n",
      "    ```\n",
      "\n",
      "\n",
      "**V. Compile and Run the Graph**\n",
      "\n",
      "1.  **Compile the Graph:**\n",
      "\n",
      "    \n",
      "```python\n",
      "    chain = graph.compile()\n",
      "    ```\n",
      "\n",
      "\n",
      "2.  **Run the Agent:** Provide the initial state (including the user's goal).\n",
      "\n",
      "    \n",
      "```python\n",
      "    inputs = {\"messages\": [], \"user_goal\": \"Find the current weather in London.\"} # initialize the messages\n",
      "    result = chain.invoke(inputs)\n",
      "    print(result)\n",
      "    ```\n",
      "\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Tool Selection:** Implement a robust mechanism for the agent to choose the right tool for the job.  This might involve using a separate LLM call to analyze the situation and select the most appropriate tool.\n",
      "*   **Error Handling:**  Handle cases where tools fail or return unexpected results.\n",
      "*   **Context Window:** Be mindful of the LLM's context window.  If the conversation gets too long, you may need to truncate the history or use a summarization technique.\n",
      "*   **Conversation History:** Store the conversation history in the `messages` part of the `AgentState`.\n",
      "*   **Security:** If you're using external tools or APIs, be sure to handle authentication and authorization securely.\n",
      "*   **Loop Prevention:** Implement mechanisms to prevent the agent from getting stuck in infinite loops. The `should_use_tool` function with the \"agent\": \"agent\" loop is a simple example, but you might need more sophisticated logic. You can also add a max iteration count.\n",
      "\n",
      "This is a basic framework. You'll likely need to iterate and refine your agent's design as you test it and identify areas for improvement. Good luck, and let me know if you have more specific questions!\n"
     ]
    }
   ],
   "source": [
    "# outputs[-1].content\n",
    "\n",
    "print('\\n'.join(outputs[-1].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I can provide a general step-by-step walkthrough to help you get started, but keep in mind that building an autonomous agent is a complex task, and the specific steps will depend on the agent\\'s purpose and design.\\n\\n**I. Project Setup and Initialization**\\n\\n1.  **Install LangGraph and LangChain:**\\n\\n     ```bash\\n    pip install langgraph langchain\\n    ``` \\n\\n2.  **Set up your environment:** Make sure you have Python installed (preferably 3.8 or higher). You\\'ll also need an API key for a Large Language Model provider like OpenAI. Set the API key as an environment variable:\\n\\n     ```bash\\n    export OPENAI_API_KEY=\"YOUR_API_KEY\"\\n    ``` \\n\\n**II. Define the Agent\\'s State**\\n\\n1.  **Define the State Class:** The state represents the agent\\'s memory and current understanding. It should include all the information the agent needs to make decisions. For example:\\n\\n     ```python\\n    from typing import TypedDict, List, Dict\\n    from langchain.schema import BaseMessage\\n\\n    class AgentState(TypedDict):\\n        messages: List[BaseMessage]\\n        user_goal: str\\n        # Add other relevant state variables like \\'tools_available\\', \\'current_task\\', etc.\\n    ``` \\n\\n**III. Define the Agent\\'s Actions (Nodes)**\\n\\n1.  **Create Tool Functions:** Define functions that the agent can use to interact with the world. These could include:\\n\\n    *   A search tool (using Tavily or another search engine).\\n    *   A writing tool (to generate text).\\n    *   A tool to access a database or API.\\n    *   A tool to reflect on its actions.\\n\\n    Example using the available `default_api`:\\n\\n     ```python\\n    from langchain.tools import tool\\n\\n    @tool\\n    def search_internet(query: str) -> str:\\n        \"\"\"Searches the internet for relevant information.\"\"\"\\n        return default_api.tavily_search_results_json(query=query)\\n    ``` \\n\\n2.  **Create Agent Logic (Nodes):** These are the functions that determine the agent\\'s behavior. They take the current state as input and return actions to take (e.g., call a tool, update the state, decide on the next step).\\n\\n     ```python\\n    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\n    from langchain.chat_models import ChatOpenAI\\n    from langchain.schema import SystemMessage, HumanMessage, AIMessage\\n\\n    # Initialize LLM\\n    model = ChatOpenAI(temperature=0.7)\\n\\n    # Define a prompt\\n    prompt = ChatPromptTemplate.from_messages([\\n        SystemMessage(\"You are a helpful AI assistant. You have access to tools to help you answer questions. Reason step by step. \"),\\n        MessagesPlaceholder(variable_name=\"messages\"),\\n        HumanMessage(\"{user_goal}\"),\\n    ])\\n\\n    def agent_node(state: AgentState):\\n        \"\"\"\\n        This node decides what to do based on the current state.\\n        \"\"\"\\n        messages = state[\\'messages\\']\\n        user_goal = state[\\'user_goal\\']\\n\\n        # Format the prompt\\n        formatted_prompt = prompt.format_messages(messages=messages, user_goal=user_goal)\\n\\n        # Get the model\\'s response\\n        response = model(formatted_prompt)\\n\\n        # Return the next action\\n        return {\"next\": response}\\n    ``` \\n\\n**IV. Define the Graph (Edges and Conditional Transitions)**\\n\\n1.  **Create the Graph:** Use `langgraph.Graph` to define the workflow.\\n\\n     ```python\\n    import langgraph as lg\\n    from langchain.schema import AgentAction, AgentFinish\\n    from typing import Union\\n\\n    # Define a Union type for the output of the agent node\\n    AgentOutput = Union[AgentAction, AgentFinish, AIMessage]\\n\\n    graph = lg.Graph[AgentState]()\\n\\n    # Add the agent node\\n    graph.add_node(\"agent\", agent_node)\\n    ``` \\n\\n2.  **Add Tool Nodes:** For each tool, create a node that executes the tool and updates the state.\\n\\n     ```python\\n    def execute_tool(state):\\n        \"\"\"Executes the tool chosen by the agent.\"\"\"\\n        messages = state[\"messages\"]\\n        # Get the last message, which should be an AgentAction\\n        last_message = messages[-1]\\n        tool_name = last_message.tool\\n        tool_input = last_message.tool_input\\n\\n        # Execute the tool (replace with your actual tool execution logic)\\n        if tool_name == \"search_internet\":\\n            tool_output = search_internet.run(tool_input) # Assuming search_internet is a LangChain Tool\\n        else:\\n            tool_output = \"Tool not found.\"\\n\\n        # Return the tool output as a message\\n        return {\"next\": AIMessage(content=str(tool_output))}\\n\\n    graph.add_node(\"tool\", execute_tool)\\n    ``` \\n\\n3.  **Define Edges and Conditional Transitions:**\\n\\n    *   **Conditional Edges:** Determine the flow of the graph based on the agent\\'s output.  For example, if the agent chooses a tool, go to the tool execution node.  If the agent is finished, end the graph.\\n\\n     ```python\\n    def should_use_tool(message: AgentOutput):\\n        \"\"\"Determines whether the agent should use a tool or respond directly.\"\"\"\\n        if isinstance(message, AgentAction):\\n            return \"tool\"\\n        elif isinstance(message, AgentFinish):\\n            return \"end\"\\n        else:\\n            return \"agent\" # Re-invoke agent if it didn\\'t return an action\\n\\n    graph.add_conditional_edges(\"agent\", should_use_tool, {\\n        \"tool\": \"tool\",\\n        \"end\": lg.END,\\n        \"agent\": \"agent\" #loop\\n    })\\n\\n    graph.add_edge(\"tool\", \"agent\") # after tool, go back to agent\\n    ``` \\n\\n4.  **Entrypoint:** Designate the starting node.\\n\\n     ```python\\n    graph.set_entry_point(\"agent\")\\n    ``` \\n\\n**V. Compile and Run the Graph**\\n\\n1.  **Compile the Graph:**\\n\\n     ```python\\n    chain = graph.compile()\\n    ``` \\n\\n2.  **Run the Agent:** Provide the initial state (including the user\\'s goal).\\n\\n     ```python\\n    inputs = {\"messages\": [], \"user_goal\": \"Find the current weather in London.\"} # initialize the messages\\n    result = chain.invoke(inputs)\\n    print(result)\\n    ``` \\n\\n**Important Considerations:**\\n\\n*   **Tool Selection:** Implement a robust mechanism for the agent to choose the right tool for the job.  This might involve using a separate LLM call to analyze the situation and select the most appropriate tool.\\n*   **Error Handling:**  Handle cases where tools fail or return unexpected results.\\n*   **Context Window:** Be mindful of the LLM\\'s context window.  If the conversation gets too long, you may need to truncate the history or use a summarization technique.\\n*   **Conversation History:** Store the conversation history in the `messages` part of the `AgentState`.\\n*   **Security:** If you\\'re using external tools or APIs, be sure to handle authentication and authorization securely.\\n*   **Loop Prevention:** Implement mechanisms to prevent the agent from getting stuck in infinite loops. The `should_use_tool` function with the \"agent\": \"agent\" loop is a simple example, but you might need more sophisticated logic. You can also add a max iteration count.\\n\\nThis is a basic framework. You\\'ll likely need to iterate and refine your agent\\'s design as you test it and identify areas for improvement. Good luck, and let me know if you have more specific questions!'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(outputs[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Convert the following into markdown -> {result}\",\n",
    "    input_variables=[\"result\"],\n",
    ")\n",
    "\n",
    "markdown_chain=prompt|model|str_parser\n",
    "result=markdown_chain.invoke({\"result\":' '.join(outputs[-1].content)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Introduction\n",
       "I am still unable to reliably extract all the required information to fulfill the request completely. However, here are some potential leads based on the search results. Please note that the information might be incomplete, and it is recommended to verify the details on the linked pages.\n",
       "\n",
       "### Job Leads\n",
       "1. **Company Name:** Observe AI\n",
       "    * **Job Role:** Machine Learning Engineer - NLP\n",
       "    * **Skills Required:** Machine Learning, Deep Learning, NLP\n",
       "    * **Location:** Not specified in the summary, check the link for details.\n",
       "    * **Salary:** Not specified\n",
       "    * **Application Link:** [https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210](https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210)\n",
       "2. **Company Name:** IQVIA\n",
       "    * **Job Role:** AI Engineer\n",
       "    * **Skills Required:** Python, TensorFlow, PyTorch, Hugging Face (likely others, see link)\n",
       "    * **Location:** Bengaluru\n",
       "    * **Salary:** Not specified\n",
       "    * **Application Link:** [https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-72324485556892444288-aVZT](https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-72324485556892444288-aVZT)\n",
       "3. **Company Name:** Coditas\n",
       "    * **Job Role:** Generative AI Engineer - Python\n",
       "    * **Skills Required:** Azure, Python, TensorFlow, PyTorch, scikit-learn\n",
       "    * **Location:** Not specified, check the link for details.\n",
       "    * **Salary:** Not specified\n",
       "    * **Application Link:** [https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-BlDP](https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-BlDP)\n",
       "\n",
       "### Conclusion\n",
       "I am continuing to improve my ability to extract this information. In the meantime, please use these leads as a starting point for your job search and verify the details on the application pages."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown,display\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input:str,config:dict=config):\n",
    "    events = agent.stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        user_input\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            outputs.extend(event[\"messages\"])\n",
    "\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"Convert the following into markdown -> {result}\",\n",
    "        input_variables=[\"result\"],\n",
    "    )\n",
    "\n",
    "    markdown_chain=prompt|model|str_parser\n",
    "    result=markdown_chain.invoke({\"result\":' '.join(outputs[-1].content)})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=get_response(\"Give me an explanation of generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generative AI\n",
       "Generative AI refers to a class of artificial intelligence algorithms that learn from existing data (text, images, audio, etc.) to generate new, similar data. It's like teaching a computer to imitate an artist, writer, or composer, and then letting it create its own original works in that style.\n",
       "\n",
       "#### Key Concepts\n",
       "* **Learning from Data**: Generative AI models are trained on large datasets. They analyze the patterns, structures, and relationships within the data. For example, a model trained on images of cats learns what features (ears, fur, whiskers, etc.) are characteristic of cats.\n",
       "* **Generating New Data**: Once trained, these models can generate new data points that resemble the training data but are not identical to it. The \"newness\" can range from slight variations on existing examples to completely original creations. In the cat example, the model could generate a picture of a cat it has never seen before.\n",
       "* **Types of Generative Models**: There are several different architectures used for generative AI, including:\n",
       "\t+ **Generative Adversarial Networks (GANs)**: GANs involve two neural networks, a generator and a discriminator. The generator creates new data, and the discriminator tries to distinguish between the generated data and real data from the training set. The generator and discriminator are trained in competition, leading to increasingly realistic generated data.\n",
       "\t+ **Variational Autoencoders (VAEs)**: VAEs learn a compressed representation of the input data (a \"latent space\"). They can then sample from this latent space and decode the samples to generate new data points.\n",
       "\t+ **Transformers**: While originally designed for natural language processing, transformers have proven highly effective for generative tasks in other domains, such as image and audio generation. They use a mechanism called \"self-attention\" to weigh the importance of different parts of the input data when generating new data.\n",
       "\t+ **Autoregressive Models**: These models predict the next element in a sequence based on the preceding elements. They are commonly used for text and music generation.\n",
       "\n",
       "#### Applications of Generative AI\n",
       "Generative AI has a wide range of applications, including:\n",
       "* **Image Generation**: Creating realistic images of people, objects, and scenes.\n",
       "* **Text Generation**: Writing articles, poems, scripts, and code.\n",
       "* **Music Generation**: Composing original music in various styles.\n",
       "* **Video Generation**: Creating short videos or animations.\n",
       "* **Drug Discovery**: Designing new molecules with desired properties.\n",
       "* **Product Design**: Generating new product ideas and prototypes.\n",
       "* **Data Augmentation**: Creating synthetic data to improve the performance of other machine learning models.\n",
       "\n",
       "#### Limitations and Challenges\n",
       "* **Data Dependency**: Generative AI models require large amounts of training data.\n",
       "* **Bias**: Generated data can reflect biases present in the training data.\n",
       "* **Lack of Control**: It can be difficult to control the specific characteristics of the generated data.\n",
       "* **Ethical Concerns**: Concerns about misuse, such as creating deepfakes or generating misleading content.\n",
       "* **Computational Cost**: Training generative models can be computationally expensive.\n",
       "\n",
       "In summary, generative AI is a powerful technology that allows computers to create new data that resembles the data they were trained on. It has numerous applications across various industries, but it also presents ethical and practical challenges that need to be addressed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3=get_response(\"make it a bit shorter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generative AI\n",
       "Generative AI refers to a class of artificial intelligence algorithms that learn from existing data (text, images, audio, etc.) to generate new, similar data. It's like teaching a computer to imitate an artist, writer, or composer, and then letting it create its own original works in that style.\n",
       "\n",
       "#### Key Concepts\n",
       "* **Learning from Data**: Generative AI models are trained on large datasets. They analyze the patterns, structures, and relationships within the data. For example, a model trained on images of cats learns what features (ears, fur, whiskers, etc.) are characteristic of cats.\n",
       "* **Generating New Data**: Once trained, these models can generate new data points that resemble the training data but are not identical to it. The \"newness\" can range from slight variations on existing examples to completely original creations. In the cat example, the model could generate a picture of a cat it has never seen before.\n",
       "* **Types of Generative Models**: There are several different architectures used for generative AI, including:\n",
       "\t+ **Generative Adversarial Networks (GANs)**: GANs involve two neural networks, a generator and a discriminator. The generator creates new data, and the discriminator tries to distinguish between the generated data and real data from the training set. The generator and discriminator are trained in competition, leading to increasingly realistic generated data.\n",
       "\t+ **Variational Autoencoders (VAEs)**: VAEs learn a compressed representation of the input data (a \"latent space\"). They can then sample from this latent space and decode the samples to generate new data points.\n",
       "\t+ **Transformers**: While originally designed for natural language processing, transformers have proven highly effective for generative tasks in other domains, such as image and audio generation. They use a mechanism called \"self-attention\" to weigh the importance of different parts of the input data when generating new data.\n",
       "\t+ **Autoregressive Models**: These models predict the next element in a sequence based on the preceding elements. They are commonly used for text and music generation.\n",
       "\n",
       "#### Applications of Generative AI\n",
       "Generative AI has a wide range of applications, including:\n",
       "* **Image Generation**: Creating realistic images of people, objects, and scenes.\n",
       "* **Text Generation**: Writing articles, poems, scripts, and code.\n",
       "* **Music Generation**: Composing original music in various styles.\n",
       "* **Video Generation**: Creating short videos or animations.\n",
       "* **Drug Discovery**: Designing new molecules with desired properties.\n",
       "* **Product Design**: Generating new product ideas and prototypes.\n",
       "* **Data Augmentation**: Creating synthetic data to improve the performance of other machine learning models.\n",
       "\n",
       "#### Limitations and Challenges\n",
       "* **Data Dependency**: Generative AI models require large amounts of training data.\n",
       "* **Bias**: Generated data can reflect biases present in the training data.\n",
       "* **Lack of Control**: It can be difficult to control the specific characteristics of the generated data.\n",
       "* **Ethical Concerns**: Concerns about misuse, such as creating deepfakes or generating misleading content.\n",
       "* **Computational Cost**: Training generative models can be computationally expensive.\n",
       "\n",
       "In summary, generative AI is a powerful technology that allows computers to create new data that resembles the data they were trained on. It has numerous applications across various industries, but it also presents ethical and practical challenges that need to be addressed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=get_response(\"give me a more brief explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generative AI\n",
       "Generative AI learns from data to create new, similar data. It's like teaching a computer to imitate and then create its own original content (text, images, etc.). \n",
       "#### Uses of Generative AI\n",
       "It's used for things like:\n",
       "* Generating images\n",
       "* Writing text\n",
       "* Composing music\n",
       "#### Requirements and Concerns\n",
       "However, it requires lots of data and raises ethical concerns."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render_markdown(text:str):\n",
    "    display(Markdown(text))\n",
    "\n",
    "render_markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2={\"configurable\":{\"thread_id\":\"2\"}}\n",
    "\n",
    "jobs=get_response(agent_prompt_template.format(resume=summary),config=config2)\n",
    "# industry_trends=agent.invoke({\"messages\":agent_prompt_template.format(resume=summary)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Introduction\n",
       "I am still unable to reliably extract all the required information to fulfill the request completely. However, here are some potential leads based on the search results. Please note that the information might be incomplete, and it is recommended to verify the details on the linked pages.\n",
       "\n",
       "### Job Leads\n",
       "\n",
       "1. **Company Name:** Observe AI\n",
       "    * **Job Role:** Machine Learning Engineer - NLP\n",
       "    * **Skills Required:** Machine Learning, Deep Learning, NLP\n",
       "    * **Location:** Not specified in the summary, check the link for details.\n",
       "    * **Salary:** Not specified\n",
       "    * **Application Link:** [https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210](https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210)\n",
       "\n",
       "2. **Company Name:** IQVIA\n",
       "    * **Job Role:** AI Engineer\n",
       "    * **Skills Required:** Python, TensorFlow, PyTorch, Hugging Face (Likely others, see link)\n",
       "    * **Location:** Bengaluru\n",
       "    * **Salary:** Not specified\n",
       "    * **Application Link:** [https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-723244855568924442828-avzt](https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-723244855568924442828-avzt)\n",
       "\n",
       "3. **Company Name:** Coditas\n",
       "    * **Job Role:** Generative AI Engineer - Python\n",
       "    * **Skills Required:** Azure, Python, TensorFlow, PyTorch, scikit-learn\n",
       "    * **Location:** Not specified, check the link for details.\n",
       "    * **Salary:** Not specified\n",
       "    * **Application Link:** [https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-blDP](https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-blDP)\n",
       "\n",
       "### Conclusion\n",
       "I am continuing to improve my ability to extract this information. In the meantime, please use these leads as a starting point for your job search and verify the details on the application pages."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: ### Introduction\nI am still unable to reliably extract all the required information to fulfill the request completely. However, here are some potential leads based on the search results. Please note that the information might be incomplete, and it is recommended to verify the details on the linked pages.\n\n### Job Leads\n\n1. **Company Name:** Observe AI\n    * **Job Role:** Machine Learning Engineer - NLP\n    * **Skills Required:** Machine Learning, Deep Learning, NLP\n    * **Location:** Not specified in the summary, check the link for details.\n    * **Salary:** Not specified\n    * **Application Link:** [https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210](https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210)\n\n2. **Company Name:** IQVIA\n    * **Job Role:** AI Engineer\n    * **Skills Required:** Python, TensorFlow, PyTorch, Hugging Face (Likely others, see link)\n    * **Location:** Bengaluru\n    * **Salary:** Not specified\n    * **Application Link:** [https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-723244855568924442828-avzt](https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-723244855568924442828-avzt)\n\n3. **Company Name:** Coditas\n    * **Job Role:** Generative AI Engineer - Python\n    * **Skills Required:** Azure, Python, TensorFlow, PyTorch, scikit-learn\n    * **Location:** Not specified, check the link for details.\n    * **Salary:** Not specified\n    * **Application Link:** [https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-blDP](https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-blDP)\n\n### Conclusion\nI am continuing to improve my ability to extract this information. In the meantime, please use these leads as a starting point for your job search and verify the details on the application pages.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:83\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\utils\\json.py:145\u001b[39m, in \u001b[36mparse_json_markdown\u001b[39m\u001b[34m(json_string, parser)\u001b[39m\n\u001b[32m    144\u001b[39m     json_str = json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match.group(\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\utils\\json.py:161\u001b[39m, in \u001b[36m_parse_json\u001b[39m\u001b[34m(json_str, parser)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\utils\\json.py:119\u001b[39m, in \u001b[36mparse_partial_json\u001b[39m\u001b[34m(s, strict)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:359\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    358\u001b[39m     kw[\u001b[33m'\u001b[39m\u001b[33mparse_constant\u001b[39m\u001b[33m'\u001b[39m] = parse_constant\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:83\u001b[39m, in \u001b[36mPydanticOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> TBaseModel:\n\u001b[32m     75\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m \u001b[33;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:97\u001b[39m, in \u001b[36mJsonOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[32m     90\u001b[39m \n\u001b[32m     91\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03m        The parsed JSON object.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:67\u001b[39m, in \u001b[36mPydanticOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m \u001b[33;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     json_object = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_obj(json_object)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:86\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     85\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output=text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOutputParserException\u001b[39m: Invalid json output: ### Introduction\nI am still unable to reliably extract all the required information to fulfill the request completely. However, here are some potential leads based on the search results. Please note that the information might be incomplete, and it is recommended to verify the details on the linked pages.\n\n### Job Leads\n\n1. **Company Name:** Observe AI\n    * **Job Role:** Machine Learning Engineer - NLP\n    * **Skills Required:** Machine Learning, Deep Learning, NLP\n    * **Location:** Not specified in the summary, check the link for details.\n    * **Salary:** Not specified\n    * **Application Link:** [https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210](https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210)\n\n2. **Company Name:** IQVIA\n    * **Job Role:** AI Engineer\n    * **Skills Required:** Python, TensorFlow, PyTorch, Hugging Face (Likely others, see link)\n    * **Location:** Bengaluru\n    * **Salary:** Not specified\n    * **Application Link:** [https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-723244855568924442828-avzt](https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-723244855568924442828-avzt)\n\n3. **Company Name:** Coditas\n    * **Job Role:** Generative AI Engineer - Python\n    * **Skills Required:** Azure, Python, TensorFlow, PyTorch, scikit-learn\n    * **Location:** Not specified, check the link for details.\n    * **Salary:** Not specified\n    * **Application Link:** [https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-blDP](https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-blDP)\n\n### Conclusion\nI am continuing to improve my ability to extract this information. In the meantime, please use these leads as a starting point for your job search and verify the details on the application pages.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "parser.parse(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Introduction\\nI am still unable to reliably extract all the required information to fulfill the request completely. However, here are some potential leads based on the search results. Please note that the information might be incomplete, and it is recommended to verify the details on the linked pages.\\n\\n### Job Leads\\n\\n1. **Company Name:** Observe AI\\n    * **Job Role:** Machine Learning Engineer - NLP\\n    * **Skills Required:** Machine Learning, Deep Learning, NLP\\n    * **Location:** Not specified in the summary, check the link for details.\\n    * **Salary:** Not specified\\n    * **Application Link:** [https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210](https://www.moaijobs.com/job/machine-learning-engineer-nlp-observe-ai-4210)\\n\\n2. **Company Name:** IQVIA\\n    * **Job Role:** AI Engineer\\n    * **Skills Required:** Python, TensorFlow, PyTorch, Hugging Face (Likely others, see link)\\n    * **Location:** Bengaluru\\n    * **Salary:** Not specified\\n    * **Application Link:** [https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-723244855568924442828-avzt](https://www.linkedin.com/posts/niranjan-m-084a4145_iqvia-now-hiring-ai-engineer-activity-723244855568924442828-avzt)\\n\\n3. **Company Name:** Coditas\\n    * **Job Role:** Generative AI Engineer - Python\\n    * **Skills Required:** Azure, Python, TensorFlow, PyTorch, scikit-learn\\n    * **Location:** Not specified, check the link for details.\\n    * **Salary:** Not specified\\n    * **Application Link:** [https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-blDP](https://www.linkedin.com/posts/coditas_hiring-now-langchain-rag-activity-7272508141416689664-blDP)\\n\\n### Conclusion\\nI am continuing to improve my ability to extract this information. In the meantime, please use these leads as a starting point for your job search and verify the details on the application pages.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'OK', 'request_id': '38e3b239-81e7-43c2-a906-560762ae043f', 'parameters': {'job_title': 'nodejs developer', 'location': 'new york', 'location_type': 'ANY', 'years_of_experience': None}, 'data': [{'location': 'New York City, NY', 'job_title': 'Nodejs Developer', 'min_salary': 111927.29, 'max_salary': 186051.34, 'median_salary': 143224.42, 'min_base_salary': 81995.46, 'max_base_salary': 130178.59, 'median_base_salary': 103315.31, 'min_additional_pay': 29931.83, 'max_additional_pay': 55872.75, 'median_additional_pay': 39909.11, 'salary_period': 'YEAR', 'salary_currency': 'USD', 'salary_count': 60, 'salaries_updated_at': '2024-06-06T23:59:59.000Z', 'publisher_name': 'Glassdoor', 'publisher_link': 'https://www.glassdoor.com/Salaries/company-salaries.htm?suggestCount=0&suggestChosen=false&sc.keyword=Nodejs%20Developer&locT=C&locId=1132348', 'confidence': 'CONFIDENT'}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://jsearch.p.rapidapi.com/estimated-salary\"\n",
    "\n",
    "querystring = {\"job_title\":\"nodejs developer\",\"location\":\"new york\",\"location_type\":\"ANY\",\"years_of_experience\":\"ALL\"}\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"6d4aea0084msh01be36a5c4c8990p14b81ajsn75774757b89d\",\n",
    "\t\"x-rapidapi-host\": \"jsearch.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'OK',\n",
       " 'request_id': '38e3b239-81e7-43c2-a906-560762ae043f',\n",
       " 'parameters': {'job_title': 'nodejs developer',\n",
       "  'location': 'new york',\n",
       "  'location_type': 'ANY',\n",
       "  'years_of_experience': None},\n",
       " 'data': [{'location': 'New York City, NY',\n",
       "   'job_title': 'Nodejs Developer',\n",
       "   'min_salary': 111927.29,\n",
       "   'max_salary': 186051.34,\n",
       "   'median_salary': 143224.42,\n",
       "   'min_base_salary': 81995.46,\n",
       "   'max_base_salary': 130178.59,\n",
       "   'median_base_salary': 103315.31,\n",
       "   'min_additional_pay': 29931.83,\n",
       "   'max_additional_pay': 55872.75,\n",
       "   'median_additional_pay': 39909.11,\n",
       "   'salary_period': 'YEAR',\n",
       "   'salary_currency': 'USD',\n",
       "   'salary_count': 60,\n",
       "   'salaries_updated_at': '2024-06-06T23:59:59.000Z',\n",
       "   'publisher_name': 'Glassdoor',\n",
       "   'publisher_link': 'https://www.glassdoor.com/Salaries/company-salaries.htm?suggestCount=0&suggestChosen=false&sc.keyword=Nodejs%20Developer&locT=C&locId=1132348',\n",
       "   'confidence': 'CONFIDENT'}]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=response.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://upwork-jobs-api2.p.rapidapi.com/active-freelance-1h\"\n",
    "\n",
    "querystring = {\"location_filter\":\"\\\"United States\\\"\",\"limit\":\"10\"}\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"6d4aea0084msh01be36a5c4c8990p14b81ajsn75774757b89d\",\n",
    "\t\"x-rapidapi-host\": \"upwork-jobs-api2.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'employerInterviewDetails': {'__typename': 'InterviewIG', 'advice': None, 'countHelpful': 0, 'difficulty': 'DIFFICULT', 'employer': {'name': 'ENTRUST Solutions Group', 'squareLogoUrl': 'https://media.glassdoor.com/sql/348371/entrust-solutions-group-squareLogo-1670521079923.png'}, 'employerResponses': [], 'experience': 'POSITIVE', 'featured': False, 'id': 19018219, 'jobTitle': {'text': 'Intern - Hourly'}, 'negotiationDescription': None, 'outcome': 'NO_OFFER', 'processDescription': 'Recruiter reached out to me over LinkedIn and asked if I was interested in an internship, then asked me for my information and set up an interview in their Denver office. A panel of engineers was conducting the interview. It was fairly straightforward, with some technical questions thrown in the mix.', 'reviewDateTime': '2018-02-01T03:57:16.780Z', 'source': 'APPLIED_ONLINE', 'userQuestions': [{'answerCount': 0, 'question': 'What are your three greatest accomplishments?'}, {'answerCount': 0, 'question': 'What is an ATS?'}]}}, 'status': True, 'message': 'Successful'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://glassdoor-real-time.p.rapidapi.com/companies/interview-details\"\n",
    "\n",
    "querystring = {\"interviewId\":\"19018219\"}\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"6d4aea0084msh01be36a5c4c8990p14b81ajsn75774757b89d\",\n",
    "\t\"x-rapidapi-host\": \"glassdoor-real-time.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'employerInterviewDetails': {'__typename': 'InterviewIG',\n",
       "   'advice': None,\n",
       "   'countHelpful': 0,\n",
       "   'difficulty': 'DIFFICULT',\n",
       "   'employer': {'name': 'ENTRUST Solutions Group',\n",
       "    'squareLogoUrl': 'https://media.glassdoor.com/sql/348371/entrust-solutions-group-squareLogo-1670521079923.png'},\n",
       "   'employerResponses': [],\n",
       "   'experience': 'POSITIVE',\n",
       "   'featured': False,\n",
       "   'id': 19018219,\n",
       "   'jobTitle': {'text': 'Intern - Hourly'},\n",
       "   'negotiationDescription': None,\n",
       "   'outcome': 'NO_OFFER',\n",
       "   'processDescription': 'Recruiter reached out to me over LinkedIn and asked if I was interested in an internship, then asked me for my information and set up an interview in their Denver office. A panel of engineers was conducting the interview. It was fairly straightforward, with some technical questions thrown in the mix.',\n",
       "   'reviewDateTime': '2018-02-01T03:57:16.780Z',\n",
       "   'source': 'APPLIED_ONLINE',\n",
       "   'userQuestions': [{'answerCount': 0,\n",
       "     'question': 'What are your three greatest accomplishments?'},\n",
       "    {'answerCount': 0, 'question': 'What is an ATS?'}]}},\n",
       " 'status': True,\n",
       " 'message': 'Successful'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': None, 'errors': {'interviewId': 'interviewId is required'}, 'status': False, 'message': 'Errors'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://glassdoor-real-time.p.rapidapi.com/companies/interview-details\"\n",
    "\n",
    "querystring = {\n",
    "    \"page\": \"1\",\n",
    "    \"location\": \"United States\",  # You can change this to \"India\", \"United Kingdom\", etc.\n",
    "    \"title\": \"Data Scientist\",    # Optional: Role/title filter\n",
    "    \"company\": \"Google\"           # Optional: Company name filter\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"x-rapidapi-key\": \"6d4aea0084msh01be36a5c4c8990p14b81ajsn75774757b89d\",\n",
    "    \"x-rapidapi-host\": \"glassdoor-real-time.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'some error ocurred', 'status': False}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://jobs-search-api.p.rapidapi.com/getjobs\"\n",
    "\n",
    "payload = {\n",
    "\t\"search_term\": \"data\",\n",
    "\t\"location\": [\"mumbai\",\"delhi\",\"kolkata\",'bengaluru',\"hyderabad\",\"new york city\"],\n",
    "\t\"results_wanted\": 5,\n",
    "\t\"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "\t\"distance\": 50,\n",
    "\t# \"job_type\": \"fulltime\",\n",
    "\t# \"is_remote\": False,\n",
    "\t\"linkedin_fetch_description\": False,\n",
    "\t\"hours_old\": 72\n",
    "}\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"6d4aea0084msh01be36a5c4c8990p14b81ajsn75774757b89d\",\n",
    "\t\"x-rapidapi-host\": \"jobs-search-api.p.rapidapi.com\",\n",
    "\t\"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'li-4200971051',\n",
       "  'site': 'linkedin',\n",
       "  'job_url': 'https://www.linkedin.com/jobs/view/4200971051',\n",
       "  'job_url_direct': '',\n",
       "  'title': 'UI Developer_Associate_Software Engineering',\n",
       "  'company': 'Morgan Stanley',\n",
       "  'location': 'Mumbai, Maharashtra, India',\n",
       "  'date_posted': '2025-04-04',\n",
       "  'job_type': '',\n",
       "  'salary_source': '',\n",
       "  'interval': '',\n",
       "  'min_amount': '',\n",
       "  'max_amount': '',\n",
       "  'currency': '',\n",
       "  'is_remote': 'False',\n",
       "  'job_level': '',\n",
       "  'job_function': '',\n",
       "  'listing_type': '',\n",
       "  'emails': '',\n",
       "  'description': '',\n",
       "  'company_industry': '',\n",
       "  'company_url': 'https://www.linkedin.com/company/morgan-stanley',\n",
       "  'company_logo': '',\n",
       "  'company_url_direct': '',\n",
       "  'company_addresses': '',\n",
       "  'company_num_employees': '',\n",
       "  'company_revenue': '',\n",
       "  'company_description': '',\n",
       "  'skills': '',\n",
       "  'experience_range': '',\n",
       "  'company_rating': '',\n",
       "  'company_reviews_count': '',\n",
       "  'vacancy_count': '',\n",
       "  'work_from_home_type': ''},\n",
       " {'id': 'li-4199480133',\n",
       "  'site': 'linkedin',\n",
       "  'job_url': 'https://www.linkedin.com/jobs/view/4199480133',\n",
       "  'job_url_direct': '',\n",
       "  'title': 'Web Developer',\n",
       "  'company': 'Outlier',\n",
       "  'location': '',\n",
       "  'date_posted': '2025-04-04',\n",
       "  'job_type': '',\n",
       "  'salary_source': '',\n",
       "  'interval': '',\n",
       "  'min_amount': '',\n",
       "  'max_amount': '',\n",
       "  'currency': '',\n",
       "  'is_remote': 'False',\n",
       "  'job_level': '',\n",
       "  'job_function': '',\n",
       "  'listing_type': '',\n",
       "  'emails': '',\n",
       "  'description': '',\n",
       "  'company_industry': '',\n",
       "  'company_url': 'https://www.linkedin.com/company/try-outlier',\n",
       "  'company_logo': '',\n",
       "  'company_url_direct': '',\n",
       "  'company_addresses': '',\n",
       "  'company_num_employees': '',\n",
       "  'company_revenue': '',\n",
       "  'company_description': '',\n",
       "  'skills': '',\n",
       "  'experience_range': '',\n",
       "  'company_rating': '',\n",
       "  'company_reviews_count': '',\n",
       "  'vacancy_count': '',\n",
       "  'work_from_home_type': ''},\n",
       " {'id': 'li-4199722929',\n",
       "  'site': 'linkedin',\n",
       "  'job_url': 'https://www.linkedin.com/jobs/view/4199722929',\n",
       "  'job_url_direct': '',\n",
       "  'title': 'Marketing Operations and Digital Associate',\n",
       "  'company': 'Morgan Stanley',\n",
       "  'location': 'Mumbai, Maharashtra, India',\n",
       "  'date_posted': '2025-04-04',\n",
       "  'job_type': '',\n",
       "  'salary_source': '',\n",
       "  'interval': '',\n",
       "  'min_amount': '',\n",
       "  'max_amount': '',\n",
       "  'currency': '',\n",
       "  'is_remote': 'False',\n",
       "  'job_level': '',\n",
       "  'job_function': '',\n",
       "  'listing_type': '',\n",
       "  'emails': '',\n",
       "  'description': '',\n",
       "  'company_industry': '',\n",
       "  'company_url': 'https://www.linkedin.com/company/morgan-stanley',\n",
       "  'company_logo': '',\n",
       "  'company_url_direct': '',\n",
       "  'company_addresses': '',\n",
       "  'company_num_employees': '',\n",
       "  'company_revenue': '',\n",
       "  'company_description': '',\n",
       "  'skills': '',\n",
       "  'experience_range': '',\n",
       "  'company_rating': '',\n",
       "  'company_reviews_count': '',\n",
       "  'vacancy_count': '',\n",
       "  'work_from_home_type': ''},\n",
       " {'id': 'li-4199479294',\n",
       "  'site': 'linkedin',\n",
       "  'job_url': 'https://www.linkedin.com/jobs/view/4199479294',\n",
       "  'job_url_direct': '',\n",
       "  'title': 'Web Developer',\n",
       "  'company': 'Outlier',\n",
       "  'location': 'Thane, Maharashtra, India',\n",
       "  'date_posted': '2025-04-04',\n",
       "  'job_type': '',\n",
       "  'salary_source': '',\n",
       "  'interval': '',\n",
       "  'min_amount': '',\n",
       "  'max_amount': '',\n",
       "  'currency': '',\n",
       "  'is_remote': 'False',\n",
       "  'job_level': '',\n",
       "  'job_function': '',\n",
       "  'listing_type': '',\n",
       "  'emails': '',\n",
       "  'description': '',\n",
       "  'company_industry': '',\n",
       "  'company_url': 'https://www.linkedin.com/company/try-outlier',\n",
       "  'company_logo': '',\n",
       "  'company_url_direct': '',\n",
       "  'company_addresses': '',\n",
       "  'company_num_employees': '',\n",
       "  'company_revenue': '',\n",
       "  'company_description': '',\n",
       "  'skills': '',\n",
       "  'experience_range': '',\n",
       "  'company_rating': '',\n",
       "  'company_reviews_count': '',\n",
       "  'vacancy_count': '',\n",
       "  'work_from_home_type': ''},\n",
       " {'id': 'li-4121157377',\n",
       "  'site': 'linkedin',\n",
       "  'job_url': 'https://www.linkedin.com/jobs/view/4121157377',\n",
       "  'job_url_direct': '',\n",
       "  'title': 'HR Ops Admin',\n",
       "  'company': 'Amazon',\n",
       "  'location': 'Thane, Maharashtra, India',\n",
       "  'date_posted': '',\n",
       "  'job_type': '',\n",
       "  'salary_source': '',\n",
       "  'interval': '',\n",
       "  'min_amount': '',\n",
       "  'max_amount': '',\n",
       "  'currency': '',\n",
       "  'is_remote': 'False',\n",
       "  'job_level': '',\n",
       "  'job_function': '',\n",
       "  'listing_type': '',\n",
       "  'emails': '',\n",
       "  'description': '',\n",
       "  'company_industry': '',\n",
       "  'company_url': 'https://www.linkedin.com/company/amazon',\n",
       "  'company_logo': '',\n",
       "  'company_url_direct': '',\n",
       "  'company_addresses': '',\n",
       "  'company_num_employees': '',\n",
       "  'company_revenue': '',\n",
       "  'company_description': '',\n",
       "  'skills': '',\n",
       "  'experience_range': '',\n",
       "  'company_rating': '',\n",
       "  'company_reviews_count': '',\n",
       "  'vacancy_count': '',\n",
       "  'work_from_home_type': ''}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[\"jobs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "role=[]\n",
    "job_url=[]\n",
    "company=[]\n",
    "location=[]\n",
    "date_posted=[]\n",
    "salary=[]\n",
    "\n",
    "for job in response.json()[\"jobs\"]:\n",
    "    role.append(job['title'])\n",
    "    job_url.append(job[\"job_url\"])\n",
    "    company.append(job[\"company\"])\n",
    "    location.append(job[\"location\"])\n",
    "    date_posted.append(job[\"date_posted\"])\n",
    "    salary.append(f\"{job[\"min_amount\"]} - {job[\"max_amount\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>role</th>\n",
       "      <th>job_url</th>\n",
       "      <th>location</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>UI Developer_Associate_Software Engineering</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4200971051</td>\n",
       "      <td>Mumbai, Maharashtra, India</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>Web Developer</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4199480133</td>\n",
       "      <td></td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>Marketing Operations and Digital Associate</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4199722929</td>\n",
       "      <td>Mumbai, Maharashtra, India</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>Web Developer</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4199479294</td>\n",
       "      <td>Thane, Maharashtra, India</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>HR Ops Admin</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4121157377</td>\n",
       "      <td>Thane, Maharashtra, India</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company                                         role  \\\n",
       "0  Morgan Stanley  UI Developer_Associate_Software Engineering   \n",
       "1         Outlier                                Web Developer   \n",
       "2  Morgan Stanley   Marketing Operations and Digital Associate   \n",
       "3         Outlier                                Web Developer   \n",
       "4          Amazon                                 HR Ops Admin   \n",
       "\n",
       "                                         job_url                    location  \\\n",
       "0  https://www.linkedin.com/jobs/view/4200971051  Mumbai, Maharashtra, India   \n",
       "1  https://www.linkedin.com/jobs/view/4199480133                               \n",
       "2  https://www.linkedin.com/jobs/view/4199722929  Mumbai, Maharashtra, India   \n",
       "3  https://www.linkedin.com/jobs/view/4199479294   Thane, Maharashtra, India   \n",
       "4  https://www.linkedin.com/jobs/view/4121157377   Thane, Maharashtra, India   \n",
       "\n",
       "  date_posted salary  \n",
       "0  2025-04-04     -   \n",
       "1  2025-04-04     -   \n",
       "2  2025-04-04     -   \n",
       "3  2025-04-04     -   \n",
       "4                 -   "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(\n",
    "    {\n",
    "        \"company\":company,\n",
    "        \"role\":role,\n",
    "        \"job_url\":job_url,\n",
    "        \"location\":location,\n",
    "        \"date_posted\":date_posted,\n",
    "        \"salary\":salary\n",
    "    }\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_jobs_rapid_api():\n",
    "    url = \"https://jobs-search-api.p.rapidapi.com/getjobs\"\n",
    "\n",
    "    payload = {\n",
    "        \"search_term\": \"web\",\n",
    "        \"location\": \"mumbai\",\n",
    "        \"results_wanted\": 5,\n",
    "        \"site_name\": [\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\"],\n",
    "        \"distance\": 50,\n",
    "        \"job_type\": \"fulltime\",\n",
    "        \"is_remote\": False,\n",
    "        \"linkedin_fetch_description\": False,\n",
    "        \"hours_old\": 72\n",
    "    }\n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": \"6d4aea0084msh01be36a5c4c8990p14b81ajsn75774757b89d\",\n",
    "        \"x-rapidapi-host\": \"jobs-search-api.p.rapidapi.com\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 0, 'hits': [], 'indeed_final_url': 'https://www.indeed.com/cmp/Ubisoft/jobs?clearPrefilter=1&start=1'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://indeed12.p.rapidapi.com/company/Ubisoft/jobs\"\n",
    "\n",
    "querystring = {\"title\":\"data-scientist\",\"start\":\"1\"}\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"6d4aea0084msh01be36a5c4c8990p14b81ajsn75774757b89d\",\n",
    "\t\"x-rapidapi-host\": \"indeed12.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=get_response(agent_prompt_template.format(resume=summary),config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Based on the search results, here are a few potential job opportunities that align with Soham Mukherjee's profile.\n",
       "Since the previous search did not give specific job postings, I have used a broader search query. Please note that due to the limitations of the search tool, I may not be able to verify the current status of these postings. Always check the application link to confirm the job is still available and to get the most accurate details.\n",
       "\n",
       "#### Job Opportunities\n",
       "1. **Company Name:** Pyxis\n",
       "\t* **Job Role:** Machine Learning Engineer\n",
       "\t* **Skills Required:** Computer Vision, Natural Language Processing (NLP), Generative AI, Deep Learning\n",
       "\t* **Location:** Not specified\n",
       "\t* **Salary:** Not specified\n",
       "\t* **Application Link:** [https://pyxis.tech/jobs/machine-learning-engineer/](https://pyxis.tech/jobs/machine-learning-engineer/)\n",
       "2. **Company Name:** F(x) Data Labs\n",
       "\t* **Job Role:** AI Engineer – LLM Specialist\n",
       "\t* **Skills Required:** Machine Learning, Generative AI, NLP, Computer Vision, Deep Learning & LLM Expertise\n",
       "\t* **Location:** Not specified\n",
       "\t* **Salary:** Not specified\n",
       "\t* **Application Link:** [https://wellfound.com/jobs/3235842-ai-engineer-llm-specialist](https://wellfound.com/jobs/3235842-ai-engineer-llm-specialist)\n",
       "3. **Company Name:** aijobs.net (Aggregator for LangChain related jobs)\n",
       "\t* **Job Role:** Various - AI Engineer, Machine Learning Engineer, and related roles\n",
       "\t* **Skills Required:** Varies depending on the specific job posting, but generally includes LangChain, AI/ML, Python\n",
       "\t* **Location:** Varies, global opportunities\n",
       "\t* **Salary:** Not specified\n",
       "\t* **Application Link:** [https://aijobs.net/list/langchain-related-jobs/](https://aijobs.net/list/langchain-related-jobs/) (This is a job board, you'll need to navigate to individual postings)\n",
       "\n",
       "### Note\n",
       "I am unable to provide 5 specific job postings with complete information at this time due to the limitations of the tool and the nature of the search results. It is recommended to use dedicated job search platforms for a more comprehensive search."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Introduction to Neural Networks\\nNeural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text, or time series, must be translated.\\n\\n### Key Aspects of Neural Networks\\n#### Core Concepts:\\n*   **Neurons (Nodes)**: The basic building blocks of a neural network. Each neuron receives input, processes it, and produces an output.\\n*   **Connections (Edges/Weights)**: Neurons are connected to each other through connections. Each connection has a weight associated with it, which represents the strength of the connection.\\n*   **Layers**: Neurons are organized into layers:\\n    *   **Input Layer**: Receives the initial data.\\n    *   **Hidden Layers**: Perform intermediate computations. A neural network can have multiple hidden layers, allowing it to learn complex patterns.\\n    *   **Output Layer**: Produces the final result.\\n*   **Activation Function**: A function applied to the output of each neuron. Activation functions introduce non-linearity, allowing neural networks to learn complex relationships in the data. Common activation functions include ReLU, sigmoid, and tanh.\\n*   **Forward Propagation**: The process of feeding input data through the network, layer by layer, to produce an output.\\n*   **Backpropagation**: The process of adjusting the weights of the connections based on the error between the predicted output and the actual output. This is done using optimization algorithms like gradient descent.\\n*   **Training**: The process of repeatedly feeding data to the network and adjusting the weights until the network learns to produce accurate outputs.\\n\\n### How Neural Networks Work (Simplified)\\n1.  **Input**: The neural network receives input data (e.g., an image, text, or sensor readings).\\n2.  **Weighted Sum**: Each neuron in the input layer passes the data to the neurons in the next layer. Each connection has a weight, so the input to a neuron in the next layer is the sum of the weighted inputs from the previous layer.\\n3.  **Activation**: The neuron applies an activation function to this weighted sum. This introduces non-linearity and determines the neuron's output.\\n4.  **Propagation**: This process repeats through each layer of the network until the output layer is reached.\\n5.  **Error Calculation**: The network's output is compared to the desired output, and an error is calculated.\\n6.  **Backpropagation**: The error is propagated back through the network, and the weights of the connections are adjusted to reduce the error.\\n7.  **Iteration**: Steps 1-6 are repeated many times with different training examples until the network learns to produce accurate outputs.\\n\\n### Types of Neural Networks\\n*   **Feedforward Neural Networks (FNNs)**: The simplest type, where data flows in one direction (from input to output).\\n*   **Convolutional Neural Networks (CNNs)**: Designed for processing images and videos. They use convolutional layers to extract features from the input data.\\n*   **Recurrent Neural Networks (RNNs)**: Designed for processing sequential data, such as text and time series. They have feedback connections that allow them to maintain a memory of past inputs.\\n*   **Long Short-Term Memory (LSTM) Networks**: A type of RNN that is better at handling long-term dependencies in sequential data.\\n*   **Transformers**: A more recent architecture that has achieved state-of-the-art results on many NLP tasks. They use attention mechanisms to weigh the importance of different parts of the input sequence.\\n*   **Generative Adversarial Networks (GANs)**: Used for generating new data that is similar to the training data. They consist of two networks: a generator and a discriminator.\\n\\n### Applications\\nNeural networks are used in a wide variety of applications, including:\\n\\n*   **Image Recognition**: Identifying objects in images and videos.\\n*   **Natural Language Processing (NLP)**: Understanding and generating human language.\\n*   **Speech Recognition**: Converting speech to text.\\n*   **Machine Translation**: Translating text from one language to another.\\n*   **Robotics**: Controlling robots and other autonomous systems.\\n*   **Medical Diagnosis**: Diagnosing diseases from medical images and other data.\\n*   **Financial Modeling**: Predicting stock prices and other financial variables.\\n\\n### Advantages\\n*   **Can learn complex patterns**: Neural networks can learn highly complex relationships in data that are difficult or impossible for traditional algorithms to learn.\\n*   **Adaptability**: They can adapt to new data and changes in the environment.\\n*   **Feature extraction**: Some types of neural networks (like CNNs) can automatically learn relevant features from the data, reducing the need for manual feature engineering.\\n\\n### Disadvantages\\n*   **Data requirements**: Neural networks typically require large amounts of data to train effectively.\\n*   **Computational cost**: Training neural networks can be computationally expensive, requiring significant processing power and time.\\n*   **Black box**: Neural networks can be difficult to interpret, making it hard to understand why they make certain predictions.\\n*   **Overfitting**: Neural networks can overfit the training data, meaning they perform well on the training data but poorly on new data.\\n\\nIn summary, neural networks are a powerful tool for machine learning, capable of solving a wide range of complex problems. They are inspired by the structure and function of the human brain and have revolutionized fields like computer vision, natural language processing, and robotics. However, they also have limitations, such as the need for large amounts of data and computational resources, and the challenge of interpretability.\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"tell me about neural networks\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
