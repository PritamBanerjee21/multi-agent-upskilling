{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "partition_image() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[image]\" (including quotes) to install the required dependencies",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_unstructured\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredLoader\n\u001b[32m      3\u001b[39m loader=UnstructuredLoader(\n\u001b[32m      4\u001b[39m     [\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresume_ss.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m     ]\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m docs=\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_unstructured\\document_loaders.py:174\u001b[39m, in \u001b[36mUnstructuredLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_path:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m load_file(f_path=f_path)\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Call _UnstructuredBaseLoader normally since file and file_path are not lists\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_unstructured\\document_loaders.py:212\u001b[39m, in \u001b[36m_SingleDocumentLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Document]:\n\u001b[32m    208\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m     elements_json = (\n\u001b[32m    210\u001b[39m         \u001b[38;5;28mself\u001b[39m._post_process_elements_json(\u001b[38;5;28mself\u001b[39m._elements_json)\n\u001b[32m    211\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.post_processors\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_elements_json\u001b[49m\n\u001b[32m    213\u001b[39m     )\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements_json:\n\u001b[32m    215\u001b[39m         metadata = \u001b[38;5;28mself\u001b[39m._get_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_unstructured\\document_loaders.py:231\u001b[39m, in \u001b[36m_SingleDocumentLoader._elements_json\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.partition_via_api:\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elements_via_api\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._convert_elements_to_dicts(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_elements_via_local\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_unstructured\\document_loaders.py:249\u001b[39m, in \u001b[36m_SingleDocumentLoader._elements_via_local\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unstructured_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata_filename\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf partitioning a fileIO object, metadata_filename must be specified\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m as well.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    247\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munstructured_kwargs\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\unstructured\\partition\\auto.py:230\u001b[39m, in \u001b[36mpartition\u001b[39m\u001b[34m(filename, file, encoding, content_type, url, headers, ssl_verify, request_timeout, strategy, skip_infer_table_types, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, data_source_metadata, metadata_filename, hi_res_model_name, model_name, starting_page_number, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m augment_metadata(elements)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_type.partitioner_shortname == \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     partition_image = \u001b[43mpartitioner_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     elements = partition_image(\n\u001b[32m    232\u001b[39m         filename=filename,\n\u001b[32m    233\u001b[39m         file=file,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         **kwargs,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m augment_metadata(elements)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\unstructured\\partition\\auto.py:362\u001b[39m, in \u001b[36m_PartitionerLoader.get\u001b[39m\u001b[34m(self, file_type)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# -- if the partitioner is not in the cache, load it; note this raises if one or more of\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# -- the partitioner's dependencies is not installed.\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._partitioners:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28mself\u001b[39m._partitioners[file_type] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_partitioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._partitioners[file_type]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\unstructured\\partition\\auto.py:371\u001b[39m, in \u001b[36m_PartitionerLoader._load_partitioner\u001b[39m\u001b[34m(self, file_type)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pkg_name \u001b[38;5;129;01min\u001b[39;00m file_type.importable_package_dependencies:\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dependency_exists(pkg_name):\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    372\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type.partitioner_function_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() is not available because one or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    373\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m more dependencies are not installed. Use:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m pip install \u001b[39m\u001b[33m\"\u001b[39m\u001b[33munstructured[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type.extra_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (including quotes)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    375\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m to install the required dependencies\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    376\u001b[39m         )\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# -- load the partitioner and return it --\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m file_type.is_partitionable  \u001b[38;5;66;03m# -- would be a programming error if this failed --\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: partition_image() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[image]\" (including quotes) to install the required dependencies"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "loader=UnstructuredLoader(\n",
    "    [\n",
    "        \"resume_ss.png\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'open_filename' from 'pdfminer.utils' (e:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\pdfminer\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredImageLoader\n\u001b[32m      2\u001b[39m loader=UnstructuredImageLoader(\u001b[33m\"\u001b[39m\u001b[33mresume_ss.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m data=\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:107\u001b[39m, in \u001b[36mUnstructuredBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Document]:\n\u001b[32m    106\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     elements = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m._post_process_elements(elements)\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33melements\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\langchain_community\\document_loaders\\image.py:49\u001b[39m, in \u001b[36mUnstructuredImageLoader._get_elements\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_image\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m partition_image(filename=\u001b[38;5;28mself\u001b[39m.file_path, **\u001b[38;5;28mself\u001b[39m.unstructured_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\unstructured\\partition\\image.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exactly_one\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlang\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_language_args\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_pdf_or_image\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PartitionStrategy\n\u001b[32m     14\u001b[39m \u001b[38;5;129m@process_metadata\u001b[39m()\n\u001b[32m     15\u001b[39m \u001b[38;5;129m@add_metadata\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;129m@add_chunking_strategy\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     **kwargs: Any,\n\u001b[32m     37\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\unstructured\\partition\\pdf.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwrapt\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LTContainer, LTImage, LTItem, LTTextBox\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m open_filename\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpi_heif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_heif_opener\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m PILImage\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'open_filename' from 'pdfminer.utils' (e:\\Major Project Sem-4\\proj-env\\Lib\\site-packages\\pdfminer\\utils.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.image import UnstructuredImageLoader\n",
    "loader=UnstructuredImageLoader(\"resume_ss.png\")\n",
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "model=genai.GenerativeModel(\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image=Image.open(\"resume_ss.png\")\n",
    "result=model.generate_content([\"Extract all the details\",image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the extracted details from the image:\n",
      "\n",
      "**Name:** Soham Mukherjee\n",
      "\n",
      "**Contact Information:**\n",
      "*   Location: Kolkata, India\n",
      "*   Phone: +91 8240771772\n",
      "*   Email: soumyapu31@gmail.com\n",
      "*   LinkedIn: [LinkedIn](https://www.linkedin.com/)\n",
      "*   Website: [Website](https://www.website.com/)\n",
      "*   Github: [Github](https://github.com/)\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "Data Science (M.Sc.) student with experience in Machine Learning, Deep Learning, and a strong focus on AI. Skilled in Python, SQL, Statistical Analysis, Neural Networks, Computer Vision, and Natural Language Processing. Passionate about using AI to solve complex, data-driven challenges.\n",
      "\n",
      "**Skills:**\n",
      "*   Python\n",
      "*   MySQL\n",
      "*   Data Cleaning\n",
      "*   Data Preprocessing\n",
      "*   Data Visualization\n",
      "*   Data Storytelling\n",
      "*   Statistical Analysis\n",
      "*   Database Design\n",
      "*   Machine Learning\n",
      "*   Neural Networks\n",
      "*   NLP\n",
      "*   Computer Vision\n",
      "*   Model Training\n",
      "*   Model Fine-tuning\n",
      "*   Problem Solving\n",
      "*   Quick Learner\n",
      "\n",
      "**Projects:**\n",
      "*   **Automatic License Plate Detector:** [GitHub](https://github.com/)\n",
      "    *   License plate detection after fine-tuning a pre-trained YOLOv8 model on our custom dataset.\n",
      "    *   Text extraction using Tesseract OCR to recognize the license plate numbers.\n",
      "    *   Visualizing detected plates and extracted text in the web interface built using Streamlit.\n",
      "\n",
      "*   **Hollywood Movie Recommender:** [GitHub](https://github.com/)\n",
      "    *   Used content-based filtering and Natural Language Processing with cosine similarity to recommend similar movies based on genres, keywords, cast, and crew data.\n",
      "    *   Integrated the TMDb API to fetch movie details and posters, with data preprocessing involving merging datasets, stemming, and text vectorization.\n",
      "    *   Displays movie recommendations through a Streamlit web app.\n",
      "\n",
      "*   **Food Detector:** [GitHub](https://github.com/)\n",
      "    *   Initial classification with a CNN baseline model.\n",
      "    *   Transfer learning with EfficientNet and ResNet, and enhanced via data augmentation.\n",
      "    *   Fine-tuned pre-trained models to get better results.\n",
      "\n",
      "*   **WhatsApp Chat Analyzer:** [GitHub](https://github.com/) | [URL](https://www.url.com/)\n",
      "    *   Preprocess WhatsApp chat data and display key statistics (total messages, words, media, links).\n",
      "    *   Visualize user activity with timelines and heatmaps.\n",
      "    *   Generate word clouds and analyze emojis using Python's WordCloud library.\n",
      "\n",
      "*   **Flights Dashboard:** [GitHub](https://github.com/)\n",
      "    *   Built using Python's Streamlit library for an easy and seamless interaction.\n",
      "    *   Fetches data from a real-time database hosted on AWS using Python's mysql.connector library.\n",
      "    *   Provides a user interface for checking what flights are available from a given city to another city (e.g., Kolkata to Bengaluru).\n",
      "\n",
      "**Certifications:**\n",
      "*   Python for Data Science & Machine Learning [Udemy (URL)](https://www.udemy.com/)\n",
      "*   TensorFlow Developer Certification [Zero to Mastery (URL)](https://www.zerotomastery.com/)\n",
      "*   MySQL Bootcamp [Udemy (URL)](https://www.udemy.com/)\n",
      "*   Certification of Statistics & Probability using Python [Infosys Springboard (URL)](https://www.infosysspringboard.com/)\n",
      "\n",
      "**Education:**\n",
      "*   Master of Science (M.Sc.) in Data Science & Analytics, NSHM College of Management & Technology (2023-present)\n",
      "*   Bachelor of Science (B.Sc.) in Data Science, iLEAD (2020-2023)\n"
     ]
    }
   ],
   "source": [
    "print(result.to_dict()['candidates'][0][\"content\"][\"parts\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are the extracted details from the image:\\n\\n**Name:** Soham Mukherjee\\n\\n**Contact Information:**\\n*   Location: Kolkata, India\\n*   Phone: +91 8240771772\\n*   Email: soumyapu31@gmail.com\\n*   LinkedIn: [LinkedIn](https://www.linkedin.com/)\\n*   Website: [Website](https://www.website.com/)\\n*   Github: [Github](https://github.com/)\\n\\n**Summary:**\\n\\nData Science (M.Sc.) student with experience in Machine Learning, Deep Learning, and a strong focus on AI. Skilled in Python, SQL, Statistical Analysis, Neural Networks, Computer Vision, and Natural Language Processing. Passionate about using AI to solve complex, data-driven challenges.\\n\\n**Skills:**\\n*   Python\\n*   MySQL\\n*   Data Cleaning\\n*   Data Preprocessing\\n*   Data Visualization\\n*   Data Storytelling\\n*   Statistical Analysis\\n*   Database Design\\n*   Machine Learning\\n*   Neural Networks\\n*   NLP\\n*   Computer Vision\\n*   Model Training\\n*   Model Fine-tuning\\n*   Problem Solving\\n*   Quick Learner\\n\\n**Projects:**\\n*   **Automatic License Plate Detector:** [GitHub](https://github.com/)\\n    *   License plate detection after fine-tuning a pre-trained YOLOv8 model on our custom dataset.\\n    *   Text extraction using Tesseract OCR to recognize the license plate numbers.\\n    *   Visualizing detected plates and extracted text in the web interface built using Streamlit.\\n\\n*   **Hollywood Movie Recommender:** [GitHub](https://github.com/)\\n    *   Used content-based filtering and Natural Language Processing with cosine similarity to recommend similar movies based on genres, keywords, cast, and crew data.\\n    *   Integrated the TMDb API to fetch movie details and posters, with data preprocessing involving merging datasets, stemming, and text vectorization.\\n    *   Displays movie recommendations through a Streamlit web app.\\n\\n*   **Food Detector:** [GitHub](https://github.com/)\\n    *   Initial classification with a CNN baseline model.\\n    *   Transfer learning with EfficientNet and ResNet, and enhanced via data augmentation.\\n    *   Fine-tuned pre-trained models to get better results.\\n\\n*   **WhatsApp Chat Analyzer:** [GitHub](https://github.com/) | [URL](https://www.url.com/)\\n    *   Preprocess WhatsApp chat data and display key statistics (total messages, words, media, links).\\n    *   Visualize user activity with timelines and heatmaps.\\n    *   Generate word clouds and analyze emojis using Python's WordCloud library.\\n\\n*   **Flights Dashboard:** [GitHub](https://github.com/)\\n    *   Built using Python's Streamlit library for an easy and seamless interaction.\\n    *   Fetches data from a real-time database hosted on AWS using Python's mysql.connector library.\\n    *   Provides a user interface for checking what flights are available from a given city to another city (e.g., Kolkata to Bengaluru).\\n\\n**Certifications:**\\n*   Python for Data Science & Machine Learning [Udemy (URL)](https://www.udemy.com/)\\n*   TensorFlow Developer Certification [Zero to Mastery (URL)](https://www.zerotomastery.com/)\\n*   MySQL Bootcamp [Udemy (URL)](https://www.udemy.com/)\\n*   Certification of Statistics & Probability using Python [Infosys Springboard (URL)](https://www.infosysspringboard.com/)\\n\\n**Education:**\\n*   Master of Science (M.Sc.) in Data Science & Analytics, NSHM College of Management & Technology (2023-present)\\n*   Bachelor of Science (B.Sc.) in Data Science, iLEAD (2020-2023)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from helper import CandidateDetails\n",
    "\n",
    "parser=PydanticOutputParser(pydantic_object=CandidateDetails)\n",
    "temp=result.to_dict()['candidates'][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "model_groq=ChatGroq(\n",
    "    model=\"qwen-2.5-32b\",\n",
    "    api_key=groq_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        You will get resume details as follows: \\n {resume_details}. You have to parse it as follows: \\n {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"resume_details\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\":parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "image_resume_chain = prompt | model_groq | parser\n",
    "r=image_resume_chain.invoke({\n",
    "    \"resume_details\":temp\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills: \n",
      "Python, MySQL, Data Cleaning, Data Preprocessing, Data Visualization, Data Storytelling, Statistical Analysis, Database Design, Machine Learning, Neural Networks, NLP, Computer Vision, Model Training, Model Fine-tuning, Problem Solving, Quick Learner\n",
      "Experience: \n",
      "Not specified in the resume\n",
      "Education: \n",
      "Master of Science (M.Sc.) in Data Science & Analytics at NSHM College of Management & Technology (2023-present); Bachelor of Science (B.Sc.) in Data Science at iLEAD (2020-2023)\n",
      "Projects: \n",
      "['Automatic License Plate Detector: Utilizes YOLOv8 model for license plate detection and Tesseract OCR for text extraction with a Streamlit web interface.', 'Hollywood Movie Recommender: Recommends similar movies using content-based filtering and NLP with cosine similarity and integrates TMDb API for fetching movie details.', 'Food Detector: Employs CNN, EfficientNet, and ResNet models for food classification with data augmentation and fine-tuning for improved results.', 'WhatsApp Chat Analyzer: Analyzes WhatsApp chat data to display user statistics and activity timelines using Python libraries.', 'Flights Dashboard: Streamlit-based dashboard fetching real-time flight data from AWS for flight availability checks.']\n",
      "Job Role: \n",
      "Data Science (M.Sc.) student with skills in Machine Learning, Deep Learning, and AI\n",
      "Location: \n",
      "Kolkata, India\n"
     ]
    }
   ],
   "source": [
    "print(f\"Skills: \\n{r.skills}\")\n",
    "print(f\"Experience: \\n{r.experience}\")\n",
    "print(f\"Education: \\n{r.education}\")\n",
    "print(f\"Projects: \\n{r.projects}\")\n",
    "print(f\"Job Role: \\n{r.job_role}\")\n",
    "print(f\"Location: \\n{r.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
